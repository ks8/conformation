{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "\n",
    "from rdkit import Chem\n",
    "from scipy import sparse\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from tap import Tap\n",
    "\n",
    "\n",
    "from conformation.data_pytorch import Data\n",
    "from conformation.distance_matrix import distmat_to_vec\n",
    "\n",
    "# noinspection PyUnresolvedReferences\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import json\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "import torch.nn as nn\n",
    "from argparse import Namespace\n",
    "from conformation.flows import NormalizingFlowModel\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from conformation.utils import to_undirected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nets2(input_dim: int, condition_dim: int, hidden_size: int) -> nn.Sequential:\n",
    "    \"\"\"\n",
    "    RealNVP \"s\" neural network definition.\n",
    "    :param condition_dim: Dimension of embeddings.\n",
    "    :param input_dim: Data input dimension.\n",
    "    :param hidden_size: Neural network hidden size.\n",
    "    :return: nn.Sequential neural network.\n",
    "    \"\"\"\n",
    "    return nn.Sequential(nn.Linear((condition_dim + 1), hidden_size), nn.LeakyReLU(), nn.Linear(hidden_size,\n",
    "                                                                                                    hidden_size),\n",
    "                         nn.LeakyReLU(), nn.Linear(hidden_size, input_dim), nn.Tanh())\n",
    "\n",
    "def nett2(input_dim: int, condition_dim: int, hidden_size: int) -> nn.Sequential:\n",
    "    \"\"\"\n",
    "    RealNVP \"t\" neural network definition.\n",
    "    :param condition_dim: Dimension of embeddings.\n",
    "    :param input_dim: Data input dimension.\n",
    "    :param hidden_size: Neural network hidden size.\n",
    "    :return: nn.Sequential neural network.\n",
    "    \"\"\"\n",
    "    return nn.Sequential(nn.Linear((condition_dim + 1), hidden_size), nn.LeakyReLU(), nn.Linear(hidden_size,\n",
    "                                                                                                    hidden_size),\n",
    "                         nn.LeakyReLU(), nn.Linear(hidden_size, input_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args(Tap):\n",
    "    \"\"\"\n",
    "    System arguments.\n",
    "    \"\"\"\n",
    "    data_path: str  # Path to metadata file\n",
    "    num_epochs: int  # Number of training epochs\n",
    "    batch_size: int = 10  # Batch size\n",
    "    lr: float = 1e-4  # Learning rate\n",
    "    hidden_size: int = 256  # Hidden size\n",
    "    num_layers: int = 10  # Number of layers\n",
    "    num_edge_features: int = 6  # Number of edge features\n",
    "    final_linear_size: int = 1024  # Size of last linear layer\n",
    "    num_vertex_features: int = 118  # Number of vertex features\n",
    "    cuda: bool = False  # Cuda availability\n",
    "    checkpoint_path: str = None  # Directory of checkpoint to load saved model\n",
    "    save_dir: str  # Save directory\n",
    "    log_frequency: int = 10  # Log frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelationalNetwork(torch.nn.Module):\n",
    "    \"\"\" Relational network definition \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size=256, num_layers=32, num_edge_features=None, num_vertex_features=None,\n",
    "                 final_linear_size=1024, cnf=True):\n",
    "        super(RelationalNetwork, self).__init__()\n",
    "        self.hidden_size = hidden_size  # Internal feature size\n",
    "        self.num_layers = num_layers  # Number of relational layers\n",
    "        self.num_edge_features = num_edge_features  # Number of input edge features\n",
    "        self.num_vertex_features = num_vertex_features  # Number of input vertex features\n",
    "        self.final_linear_size = final_linear_size  # Number of nodes in final linear layer\n",
    "        self.edge_featurize = torch.nn.Linear(self.num_edge_features,\n",
    "                                              self.hidden_size)  # Initial linear layer for featurization of edge feat.\n",
    "        self.vertex_featurize = torch.nn.Linear(self.num_vertex_features,\n",
    "                                                self.hidden_size)  # Initial layer for featurization of vertex features\n",
    "        self.L_e = torch.nn.ModuleList([torch.nn.Linear(self.hidden_size, self.hidden_size) for _ in\n",
    "                                        range(self.num_layers)])  # Linear layers for edges\n",
    "        self.L_v = torch.nn.ModuleList([torch.nn.Linear(self.hidden_size, self.hidden_size) for _ in\n",
    "                                        range(self.num_layers)])  # Linear layers for vertices\n",
    "        self.edge_batch_norm = torch.nn.ModuleList(\n",
    "            [torch.nn.BatchNorm1d(self.hidden_size) for _ in range(self.num_layers)])  # Batch norms for edges (\\phi_e)\n",
    "        self.vertex_batch_norm = torch.nn.ModuleList([torch.nn.BatchNorm1d(self.hidden_size) for _ in\n",
    "                                                      range(self.num_layers)])  # Batch norms for vertices (\\phi_v)\n",
    "        self.gru = torch.nn.ModuleList(\n",
    "            [torch.nn.GRU(self.hidden_size, self.hidden_size) for _ in range(self.num_layers)])  # GRU cells\n",
    "        self.final_linear_layer = torch.nn.Linear(self.hidden_size, self.final_linear_size)  # Final linear layer\n",
    "        self.output_layer = torch.nn.Linear(self.final_linear_size, 1)  # Output layer\n",
    "        self.cnf = cnf\n",
    "\n",
    "    def forward(self, batch):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        :param batch: Data batch.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        e_ij_in = self.edge_featurize(batch.edge_attr)  # Featurization\n",
    "        v_i_in = self.vertex_featurize(batch.x)\n",
    "\n",
    "        for k in range(self.num_layers):\n",
    "            e_ij = self.L_e[k](e_ij_in)  # Linear layer for edges\n",
    "            v_i_prime = self.L_v[k](v_i_in)  # Linear layer for vertices\n",
    "            e_ij_prime = F.relu(self.edge_batch_norm[k](torch.stack(\n",
    "                [e_ij[edge_num] + v_i_prime[batch.edge_index[0][edge_num]] + v_i_prime[batch.edge_index[1][edge_num]]\n",
    "                 for edge_num in range(\n",
    "                    e_ij.size(0))])))  # Add pairwise vertex features to edge features followed by batch norm and ReLU\n",
    "            undirected_edge_index = to_undirected(batch.edge_index,\n",
    "                                                  batch.num_nodes)  # Full set of undirected edges for bookkeeping\n",
    "            # noinspection PyTypeChecker\n",
    "            v_i_e = torch.stack([torch.max(e_ij_prime[np.array([np.intersect1d(\n",
    "                np.where(batch.edge_index[0] == min(vertex_num, i)),\n",
    "                np.where(batch.edge_index[1] == max(vertex_num, i))) for i in np.array(\n",
    "                undirected_edge_index[1][np.where(undirected_edge_index[0] == vertex_num)])]).flatten()], 0)[0] for\n",
    "                                 vertex_num in\n",
    "                                 range(batch.num_nodes)])  # Aggregate edge features\n",
    "            gru_input = v_i_e.view(1, batch.num_nodes, self.hidden_size)  # Resize GRU input\n",
    "            gru_hidden = v_i_in.view(1, batch.num_nodes, self.hidden_size)  # Resize GRU hidden\n",
    "            gru_output, _ = self.gru[k](gru_input, gru_hidden)  # Compute GRU output\n",
    "            v_i_c = F.relu(self.vertex_batch_norm[k](\n",
    "                gru_output.view(batch.num_nodes, self.hidden_size)))  # Apply batch norm and ReLU to GRU output\n",
    "            v_i_in = v_i_c + v_i_in  # Add residual connection to vertex input\n",
    "            e_ij_in = e_ij_prime + e_ij_in  # Add residual connection to edge input\n",
    "\n",
    "        e_ij_final = self.final_linear_layer(e_ij_in)  # Compute final linear layer\n",
    "        preds = self.output_layer(e_ij_final)  # Output layer\n",
    "\n",
    "        if self.cnf:\n",
    "            return e_ij_in\n",
    "        else:\n",
    "            return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = torch.load(\"model-99.pt\", map_location=lambda storage, loc: storage)\n",
    "loaded_args = Args().from_dict(state['args'])\n",
    "loaded_state_dict = state['state_dict']\n",
    "\n",
    "model = RelationalNetwork(loaded_args.hidden_size, loaded_args.num_layers, loaded_args.num_edge_features,\n",
    "                          loaded_args.num_vertex_features, loaded_args.final_linear_size, cnf=True)\n",
    "model.load_state_dict(loaded_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNF(nn.Module):\n",
    "    \"\"\"\n",
    "    Performs a single layer of the RealNVP flow.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nets: nn.Sequential, nett: nn.Sequential, mask: torch.Tensor, prior: MultivariateNormal, padding_dim: int) -> None:\n",
    "        \"\"\"\n",
    "        :param nets: \"s\" neural network definition.\n",
    "        :param nett: \"t\" neural network definition.\n",
    "        :param mask: Mask identifying which components of the vector will be processed together in any given layer.\n",
    "        :param prior: Base distribution.\n",
    "        :return: None.\n",
    "        \"\"\"\n",
    "        super(CNF, self).__init__()\n",
    "        self.prior = prior\n",
    "        self.mask = nn.Parameter(mask, requires_grad=False)\n",
    "        self.t = nett\n",
    "        self.s = nets\n",
    "        self.padding_dim = padding_dim\n",
    "\n",
    "    def forward(self, z: torch.Tensor, c: torch.Tensor, num: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Transform a sample from the base distribution or previous layer.\n",
    "        :param c: Condition tensor.\n",
    "        :param z: Sample from the base distribution or previous layer.\n",
    "        :return: Processed sample (in the direction towards the target distribution).\n",
    "        \"\"\"\n",
    "        if self.mask[0] == 1.0:\n",
    "            mask = [torch.from_numpy(np.array([j < int(num[i]/2) for j in range(num[i].item())]).astype(np.float32)) for i in range(len(num))]\n",
    "        else:\n",
    "            mask = [torch.from_numpy(np.array([j >= int(num[i]/2) for j in range(num[i].item())]).astype(np.float32)) for i in range(len(num))]\n",
    "        \n",
    "        for i in range(len(mask)):\n",
    "                padding = np.zeros(self.padding_dim)\n",
    "                padding[:mask[i].shape[0]] = mask[i]\n",
    "                mask[i] = padding\n",
    "        mask = nn.Parameter(torch.tensor(mask, dtype=torch.float32), requires_grad=False)\n",
    "        x = z\n",
    "        x_ = x * mask\n",
    "        c_ = c*mask.unsqueeze(2).repeat(1, 1, c.shape[2])\n",
    "        combine = torch.cat((c_, x_.unsqueeze(2)), axis=2)\n",
    "        combine_ = combine        \n",
    "#         combine_ = combine.view(combine.shape[0], -1)\n",
    "        s = self.s(combine_).sum(dim=2) * (1 - mask)\n",
    "        t = self.t(combine_).sum(dim=2) * (1 - mask)\n",
    "        x = x_ + (1 - mask) * (x * torch.exp(s) + t)\n",
    "        return x\n",
    "\n",
    "    def inverse(self, x: torch.Tensor, c: torch.Tensor, num: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the inverse of a target sample or a sample from the next layer.\n",
    "        :param c: Condition tensor.\n",
    "        :param x: Sample from the target distribution or the next layer.\n",
    "        :return: Inverse sample (in the direction towards the base distribution).\n",
    "        \"\"\"\n",
    "        if self.mask[0] == 1.0:\n",
    "            mask = [torch.from_numpy(np.array([j < int(num[i]/2) for j in range(num[i].item())]).astype(np.float32)) for i in range(len(num))]\n",
    "        else:\n",
    "            mask = [torch.from_numpy(np.array([j >= int(num[i]/2) for j in range(num[i].item())]).astype(np.float32)) for i in range(len(num))]\n",
    "\n",
    "        for i in range(len(mask)):\n",
    "            padding = np.zeros(self.padding_dim)\n",
    "            padding[:mask[i].shape[0]] = mask[i]\n",
    "            mask[i] = padding\n",
    "        \n",
    "        mask = nn.Parameter(torch.tensor(mask, dtype=torch.float32), requires_grad=False)\n",
    "        log_det_j, z = x.new_zeros(x.shape[0]), x\n",
    "        z_ = mask * z\n",
    "        c_ = c*mask.unsqueeze(2).repeat(1, 1, c.shape[2])\n",
    "        combine = torch.cat((c_, z_.unsqueeze(2)), axis=2)\n",
    "        combine_ = combine\n",
    "#         combine_ = combine.view(combine.shape[0], -1)\n",
    "        s = self.s(combine_).sum(dim=2) * (1 - mask)\n",
    "        t = self.t(combine_).sum(dim=2) * (1 - mask)\n",
    "        z = (1 - mask) * (z - t) * torch.exp(-s) + z_\n",
    "        return z\n",
    "\n",
    "    def log_abs_det_jacobian(self, x: torch.Tensor, c: torch.Tensor, num: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the logarithm of the absolute value of the determinant of the Jacobian for a sample in the forward\n",
    "        direction.\n",
    "        :param c: Condition tensor.\n",
    "        :param x: Sample.\n",
    "        :return: log abs det jacobian.\n",
    "        \"\"\"\n",
    "        if self.mask[0] == 1.0:\n",
    "            mask = [torch.from_numpy(np.array([j < int(num[i]/2) for j in range(num[i])]).astype(np.float32)) for i in range(len(num))]\n",
    "        else:\n",
    "            mask = [torch.from_numpy(np.array([j >= int(num[i]/2) for j in range(num[i])]).astype(np.float32)) for i in range(len(num))]\n",
    "\n",
    "        for i in range(len(mask)):\n",
    "            padding = np.zeros(self.padding_dim)\n",
    "            padding[:mask[i].shape[0]] = mask[i]\n",
    "            mask[i] = padding\n",
    "        \n",
    "        mask = nn.Parameter(torch.tensor(mask, dtype=torch.float32), requires_grad=False)\n",
    "        log_det_j, z = x.new_zeros(x.shape[0]), x\n",
    "        z_ = mask * z\n",
    "        c_ = c*mask.unsqueeze(2).repeat(1, 1, c.shape[2])\n",
    "        combine = torch.cat((c_, z_.unsqueeze(2)), axis=2)\n",
    "        combine_ = combine\n",
    "#         combine_ = combine.view(combine.shape[0], -1)\n",
    "        s = self.s(combine_).sum(dim=2) * (1 - mask)\n",
    "        log_det_j += s.sum(dim=1)\n",
    "        return log_det_j\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNFDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for loading atomic pairwise distance information for molecules.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, metadata: List[Dict[str, str]], graph_model: RelationalNetwork, padding_dim: int, atom_types: List[int] = None, bond_types: List[float] = None):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.metadata = metadata\n",
    "        self.graph_model = graph_model\n",
    "        if bond_types is None:\n",
    "            self.bond_types = [0., 1., 1.5, 2., 3.]\n",
    "        if atom_types is None:\n",
    "            self.atom_types = [1, 6, 7, 8, 9]\n",
    "        self.padding_dim = padding_dim\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
    "        _, data = distmat_to_vec(self.metadata[idx]['path'])\n",
    "        smiles = self.metadata[idx]['smiles']\n",
    "        dist_vec = torch.from_numpy(data)\n",
    "        dist_vec = dist_vec.type(torch.float32)        \n",
    "        \n",
    "        data = Data()  # Create data object\n",
    "\n",
    "        # Molecule from SMILES string\n",
    "        smiles = self.metadata[idx]['smiles']  # Read smiles string\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        mol = Chem.AddHs(mol)\n",
    "        num_atoms = mol.GetNumAtoms()\n",
    "\n",
    "        # Compute edge connectivity in COO format corresponding to a complete graph on num_nodes\n",
    "        complete_graph = np.ones([num_atoms, num_atoms])  # Create an auxiliary complete graph\n",
    "        complete_graph = np.triu(complete_graph, k=1)  # Compute an upper triangular matrix of the complete graph\n",
    "        complete_graph = sparse.csc_matrix(complete_graph)  # Compute a csc style sparse matrix from this graph\n",
    "        row, col = complete_graph.nonzero()  # Extract the row and column indices corresponding to non-zero entries\n",
    "        row = torch.tensor(row, dtype=torch.long)\n",
    "        col = torch.tensor(col, dtype=torch.long)\n",
    "        data.edge_index = torch.stack([row, col])  # Edge connectivity in COO format (all possible edges)\n",
    "\n",
    "        # Edge features\n",
    "        # Create one-hot encoding\n",
    "        one_hot_bond_features = np.zeros((len(self.bond_types), len(self.bond_types)))\n",
    "        np.fill_diagonal(one_hot_bond_features, 1.)\n",
    "        bond_to_one_hot = dict()\n",
    "        for i in range(len(self.bond_types)):\n",
    "            bond_to_one_hot[self.bond_types[i]] = one_hot_bond_features[i]\n",
    "\n",
    "        # Extract atom indices participating in bonds and bond types\n",
    "        bonds = []\n",
    "        bond_types = []\n",
    "        for bond in mol.GetBonds():\n",
    "            bonds.append([bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()])\n",
    "            bond_types.append([bond_to_one_hot[bond.GetBondTypeAsDouble()]])\n",
    "\n",
    "        # Compute edge attributes: 1 indicates presence of bond, 0 no bond. This is concatenated with one-hot bond feat.\n",
    "        full_edges = [list(data.edge_index[:, i].numpy()) for i in range(data.edge_index.shape[1])]\n",
    "        no_bond = np.concatenate([np.array([0]), bond_to_one_hot[0]])\n",
    "        a = np.array([1])\n",
    "        edge_attr = [np.concatenate([a, bond_types[bonds.index(full_edges[i])][0]]) if full_edges[i] in bonds else\n",
    "                     no_bond for i in range(len(full_edges))]\n",
    "        data.edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "\n",
    "        # Vertex features: one-hot representation of atomic number\n",
    "        # Create one-hot encoding\n",
    "        one_hot_vertex_features = np.zeros((len(self.atom_types), len(self.atom_types)))\n",
    "        np.fill_diagonal(one_hot_vertex_features, 1.)\n",
    "        atom_to_one_hot = dict()\n",
    "        for i in range(len(self.atom_types)):\n",
    "            atom_to_one_hot[self.atom_types[i]] = one_hot_vertex_features[i]\n",
    "\n",
    "        # one_hot_vertex_features = np.zeros((self.max_atomic_num, self.max_atomic_num))\n",
    "        # np.fill_diagonal(one_hot_vertex_features, 1.)\n",
    "        one_hot_features = np.array([atom_to_one_hot[atom.GetAtomicNum()] for atom in mol.GetAtoms()])\n",
    "        data.x = torch.tensor(one_hot_features, dtype=torch.float) \n",
    "        \n",
    "        condition = self.graph_model(data).squeeze(1)\n",
    "        padding = torch.zeros([self.padding_dim, 256])\n",
    "        padding[0:condition.shape[0], :] = condition\n",
    "        condition = padding\n",
    "\n",
    "        num_dist = torch.tensor(dist_vec.shape[0])\n",
    "        \n",
    "        padding = torch.zeros(self.padding_dim)\n",
    "        padding[:dist_vec.shape[0]] = dist_vec\n",
    "        dist_vec = padding\n",
    "\n",
    "        return dist_vec, condition, num_dist \n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return '{}({})'.format(self.__class__.__name__, len(self))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = torch.load(\"model-99.pt\", map_location=lambda storage, loc: storage)\n",
    "loaded_args = Args().from_dict(state['args'])\n",
    "loaded_state_dict = state['state_dict']\n",
    "\n",
    "model = RelationalNetwork(loaded_args.hidden_size, loaded_args.num_layers, loaded_args.num_edge_features,\n",
    "                          loaded_args.num_vertex_features, loaded_args.final_linear_size)\n",
    "model.load_state_dict(loaded_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "metadata = json.load(open(\"metadata/metadata.json\"))\n",
    "train_data = CNFDataset(metadata, model, 528)\n",
    "train_data = DataLoader(train_data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = iter(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(train_data)\n",
    "x, c, num = data[0], data[1], data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base distribution\n",
    "base_dist = MultivariateNormal(torch.zeros(528), torch.eye(528))\n",
    "flow = CNF(nets2(528, 256, 256), nett2(528, 256, 256), torch.from_numpy(np.array([j >= int(28/2) for j in\n",
    "                                                             range(28)]).astype(np.float32)), base_dist, 528)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = flow(x, c, num)\n",
    "inv = flow.inverse(z, c, num)\n",
    "log_abs_det = flow.log_abs_det_jacobian(x, c, num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.4769, 1.0819, 1.0874,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [1.5086, 2.4952, 2.4569,  ..., 1.7383, 4.4792, 4.6842],\n",
       "         [1.4329, 1.0972, 1.1054,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [1.5001, 2.5477, 2.4571,  ..., 1.7705, 3.7356, 4.5172],\n",
       "         [1.4757, 1.0996, 1.1901,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [1.5200, 2.4217, 2.5123,  ..., 1.7269, 4.4235, 4.3279]]),\n",
       " tensor([[1.4769, 1.0819, 1.0874,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [1.5086, 2.4952, 2.4569,  ..., 1.7383, 4.4792, 4.6842],\n",
       "         [1.4329, 1.0972, 1.1054,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [1.5001, 2.5477, 2.4571,  ..., 1.7705, 3.7356, 4.5172],\n",
       "         [1.4757, 1.0996, 1.1901,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [1.5200, 2.4217, 2.5123,  ..., 1.7269, 4.4235, 4.3279]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " tensor([-1.5182,  7.1061, -1.5184,  7.0972, -1.5195,  7.1051, -1.5184,  7.0776,\n",
       "         -1.5242,  7.1318], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, inv, log_abs_det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7815,  1.1466,  1.2161,  ..., -0.0106,  0.0567, -0.0506],\n",
       "        [ 1.8438,  1.9414,  2.6195,  ...,  2.0657,  2.5292,  4.2434],\n",
       "        [ 1.7313,  1.1643,  1.2346,  ..., -0.0105,  0.0567, -0.0507],\n",
       "        ...,\n",
       "        [ 1.8328,  1.9849,  2.6165,  ...,  2.0976,  2.0569,  4.0927],\n",
       "        [ 1.7802,  1.1669,  1.3224,  ..., -0.0108,  0.0569, -0.0506],\n",
       "        [ 1.8476,  1.8752,  2.6724,  ...,  2.0518,  2.4952,  3.9216]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = nets2(528, 256, 256)\n",
    "t = nett2(528, 256, 256)\n",
    "\n",
    "mask = [torch.from_numpy(np.array([j < int(num[i]/2) for j in range(num[i].item())]).astype(np.float32)) for i in range(len(num))]\n",
    "\n",
    "for i in range(len(mask)):\n",
    "        padding = np.zeros(528)\n",
    "        padding[:mask[i].shape[0]] = mask[i]\n",
    "        mask[i] = padding\n",
    "mask = nn.Parameter(torch.tensor(mask, dtype=torch.float32), requires_grad=False)\n",
    "x = z\n",
    "x_ = x * mask\n",
    "c_ = c*mask.unsqueeze(2).repeat(1, 1, c.shape[2])\n",
    "combine = torch.cat((c_, x_.unsqueeze(2)), axis=2)\n",
    "combine_ = combine.view(combine.shape[0], -1)\n",
    "s_ = s(combine_) * (1 - mask)\n",
    "t_ = t(combine_) * (1 - mask)\n",
    "x = x_ + (1 - mask) * (x * torch.exp(s_) + t_)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 528, 257])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 528])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 528])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = torch.nn.Linear(257, 256)\n",
    "test(combine).sum(dim=2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conformation",
   "language": "python",
   "name": "conformation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
